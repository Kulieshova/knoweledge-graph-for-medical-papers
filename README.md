# Knowledge Graph For Mental Health Treatment Decision Support

This repository contains tools and methodologies for evaluating Knowledge Graphs (KGs) generated by Large Language Models (LLMs) against manually annotated ground truth data. It also includes a user interface (UI) for managing KG data, as well as prompt engineering and evaluation techniques.

#### Authored by **Rishika Srinivas** (@rishikasrinivas), **Nataliia Kulieshova** (@Kulieshova), **Anushka Limaye** (@anushkalimaye), **Kymari Bratton** (@Kymari28), **Fernanda Del Toro** (@Fernandadeltoro)
---

## Ground Truth Annotation  

Hand annotations were meticulously developed by team members, who manually reviewed every sentence across three provided PDFs. These annotations formed triplets structured as:  
- `subj` (Subject)  
- `relationship` (Relationship)  
- `obj` (Object)  

---

## User Interface (UI)  

### Features  
1. **Uploading Files:**
   ![Animated Demo](assets/uploading.gif)
   Users can upload PDF or CSV files for processing into KG format.  

3. **Fetching Previous KGs:**  
   ![Animated Demo](assets/fetching.gif)
   Retrieve previously saved Knowledge Graphs for continued analysis.  

4. **Searching the KG:**
   ![Animated Demo](assets/searching.gif)
   Perform searches on the KG to find specific triplets or related entities.  

5. **Reading the KG:**
   ![Animated Demo](assets/visualization.gif)
   Visualize the KG with clear distinctions using color coding and edge weights.  

### Visualization Features  
- **Color Coding:**
   
   Visual cues for different entity types and relationships.  
- **Edge Weights:**
     VISUALS TBA
   Representing the strength of relationships.  

6. **Fitering Results the KG:**
   ![Animated Demo](assets/filtering.gif)

7. **Get Help:**
   ![Animated Demo](assets/help.gif)

---

## Evaluation Methods  

### Evaluation Metrics Table  

| **Method**               | **Type**       | **Accuracy** | **Key Pitfalls**                   |  
|---------------------------|----------------|--------------|-------------------------------------|  
| Fuzzy Wuzzy              | Statistical    | 35.32%       | Low accuracy, only simple matches. |  
| TF-IDF + Cosine Similarity | Statistical    | 36.28%       | Limited to vectorized text formats.|  
| GPT Critic               | Model-Based    | 61.66%       | Requires large computational resources.|  
| G-Eval                   | Model-Based    | TBD          | Requires large computational resources.    |  
| PyTorch + bioBERT        | Model-Based    | TBD          | TBD    |  

---

### Detailed Evaluation Methodologies  

#### 1. **Fuzzy Wuzzy**  
- **Evaluation Type:** Statistical  
- **Method:**  
   - Compare each row of the ground truth to each row of LLM output.  
   - Threshold for “matching” requires 70% or above similarity.  
- **Accuracy:** 35.32%  
- **Output:**  
   - Rows of LLM output that match the ground truth at or above 70% similarity.  
   - Only one triplet pair is found matching per threshold.  

---

#### 2. **TF-IDF Vector and Cosine Method**  
- **Evaluation Type:** Statistical, Feature-weighting  
- **Method:**  
   - Combine the triplet columns into a single string.  
   - Vectorize text using TF-IDF to convert it to numeric form.  
   - Compare each LLM row to each ground truth row using cosine similarity.  
- **Accuracy:** 36.28%  
- **Output:** Best matching ground truth row for each LLM output row.  

---

#### 3. **GPT Critic Parallel Batch Request**  
- **Evaluation Type:** Model-Based  
- **Method:**  
   - Uses 10 worker threads to enable parallel comparisons.  
   - Compares each LLM output row with ground truth rows using GPT-4 (RLHF).  
   - Finds the best similarity score for each LLM output.  
- **Accuracy:** 61.66%  
- **Output:** Best ground truth match for each LLM output row.  

---

#### 4. **G-Eval**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD  

---

#### 5. **PyTorch + bioBERT**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD

#### 6. **Precision** 
- **Evaluation Type:** Statistical: word match and cosine similarity 
- **Method:** Checking if the extracted relationship is in the source text or in the ground truth annotations  
- **Threshold for Matching:** 0.7 
- **Precision Score:** 85.85

#### 7. **Hallucination** 
- **Evaluation Type:** TBA
- **Method:** TBA
- **Threshold for Matching:** TBA 
- **Hallucination Score:** TBA


---

## Contribution  
We welcome contributions to improve the evaluation methods, refine the UI, or expand the dataset. Please feel free to submit issues or pull requests.  

