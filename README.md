# Knowledge Graph For Mental Health Treatment Decision Support

This repository contains tools and methodologies for evaluating Knowledge Graphs (KGs) generated by Large Language Models (LLMs) against manually annotated ground truth data. It also includes a user interface (UI) for managing KG data, as well as prompt engineering and evaluation techniques.

#### Authored by **Rishika Srinivas** (@rishikasrinivas), **Nataliia Kulieshova** (@Kulieshova), **Anushka Limaye** (@anushkalimaye), **Kymari Bratton** (@Kymari28), **Fernanda Del Toro** (@Fernandadeltoro)
---

## Ground Truth Annotation  

Hand annotations were meticulously developed by team members, who manually reviewed every sentence across three provided PDFs. These annotations formed triplets structured as:  
- `subj` (Subject)  
- `relationship` (Relationship)  
- `obj` (Object)  

---

## User Interface (UI)

### Features

1. **Upload Files**
<p align="center">
  <img src="assets/uploading.gif" alt="File Upload Demo" width="50%">
</p>
   Easily import PDFs, which are converted into Knowledge Graphs (KGs) that extract clinical entities and relationships.

3. **Fetch Previous Graphs**  
   Retrieve saved Knowledge Graphs for continued analysis or updates.

4. **Search and Highlight**
<p align="center">
  <img src="assets/searching.gif" alt="Search Functionality Demo" width="50%">
</p>
   A **search-first design** lets users quickly locate nodes or relationships. Results are **highlighted in orange** and zoomed in for clarity.

6. **Dynamic Visualization**  
   - Nodes represent clinical entities, and edges use **color coding and varying thickness** to show relationship categories and strength.  
   - The **Relationship Table** offers a legend with clickable colored circles for more details on each relationship.  
   - A **magnitude table** shows the significance of relationships.

7. **Custom Filtering**  
   Users can **filter** the graph to focus on specific relationship types (e.g., "Side Effects" or "Recommendations"), improving clarity without clutter.

8. **Help Button**  
   An intuitive **Help button** offers guidance on the graph’s features, ensuring accessibility for new users and clinicians unfamiliar with knowledge graphs.

---

### Design Highlights

- **Brightside Health Branding**: The design aligns with **Brightside Health’s brand** using calming blues, pastels, and creative accents like **color-coded edges** for a clean, engaging experience.
- **User-Centered Design**: Focuses on usability with:  
   - **Interactive Relationship Table** for easy data interpretation.  
   - **Edge Thickness** to prioritize strong relationships for evidence-based decisions.  
   - **Search and Filtering** for focused, efficient navigation.

---

## Evaluation Methods  

### Evaluation Metrics Table  

| **Method**               | **Type**       | **Accuracy** | **Key Pitfalls**                   |  
|---------------------------|----------------|--------------|-------------------------------------|  
| Fuzzy Wuzzy              | Statistical    | 35.32%       | Low accuracy, only simple matches. |  
| TF-IDF + Cosine Similarity | Statistical    | 36.28%       | Limited to vectorized text formats.|  
| GPT Critic               | Model-Based    | 61.66%       | Requires large computational resources.|  
| G-Eval                   | Model-Based    | TBD          | Requires large computational resources.    |  
| PyTorch + bioBERT        | Model-Based    | TBD          | TBD    |  

---

### Detailed Evaluation Methodologies  

#### 1. **Fuzzy Wuzzy**  
- **Evaluation Type:** Statistical  
- **Method:**  
   - Compare each row of the ground truth to each row of LLM output.  
   - Threshold for “matching” requires 70% or above similarity.  
- **Accuracy:** 35.32%  
- **Output:**  
   - Rows of LLM output that match the ground truth at or above 70% similarity.  
   - Only one triplet pair is found matching per threshold.  

---

#### 2. **TF-IDF Vector and Cosine Method**  
- **Evaluation Type:** Statistical, Feature-weighting  
- **Method:**  
   - Combine the triplet columns into a single string.  
   - Vectorize text using TF-IDF to convert it to numeric form.  
   - Compare each LLM row to each ground truth row using cosine similarity.  
- **Accuracy:** 36.28%  
- **Output:** Best matching ground truth row for each LLM output row.  

---

#### 3. **GPT Critic Parallel Batch Request**  
- **Evaluation Type:** Model-Based  
- **Method:**  
   - Uses 10 worker threads to enable parallel comparisons.  
   - Compares each LLM output row with ground truth rows using GPT-4 (RLHF).  
   - Finds the best similarity score for each LLM output.  
- **Accuracy:** 61.66%  
- **Output:** Best ground truth match for each LLM output row.  

---

#### 4. **G-Eval**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD  

---

#### 5. **PyTorch + bioBERT**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD

#### 6. **Precision** 
- **Evaluation Type:** Statistical: word match and cosine similarity 
- **Method:** Checking if the extracted relationship is in the source text or in the ground truth annotations  
- **Threshold for Matching:** 0.7 
- **Precision Score:** 85.85

#### 7. **Hallucination** 
- **Evaluation Type:** TBA
- **Method:** TBA
- **Threshold for Matching:** TBA 
- **Hallucination Score:** TBA


---

## Contribution  
We welcome contributions to improve the evaluation methods, refine the UI, or expand the dataset. Please feel free to submit issues or pull requests.  

