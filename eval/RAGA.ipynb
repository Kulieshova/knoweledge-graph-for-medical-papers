{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGA Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Processing 110 LLM outputs against 176 ground truth statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating RAGA scores: 100%|███████████████████████████████████████████████████████████████████████████████████| 110/110 [04:07<00:00,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average RAGA Scores:\n",
      "Retrieval: 0.49\n",
      "Augmentation: 0.67\n",
      "Generation: 0.78\n",
      "Attribution: 0.63\n",
      "\n",
      "Results saved to Results/raga_evaluation_results.csv\n",
      "Processed 106 valid evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "def extract_raga_scores(evaluation):\n",
    "    \"\"\"Extract RAGA scores from GPT evaluation\"\"\"\n",
    "    try:\n",
    "        scores = {}\n",
    "        for metric in ['retrieval', 'augmentation', 'generation', 'attribution']:\n",
    "            pattern = f\"{metric}:\\\\s*([0-1](?:\\\\.\\\\d+)?)\"\n",
    "            match = re.search(pattern, evaluation.lower())\n",
    "            scores[metric] = float(match.group(1)) if match else -1\n",
    "        return scores\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting scores: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_relevant(ground_truth, llm_output):\n",
    "    \"\"\"Check if LLM output is relevant to ground truth\"\"\"\n",
    "    gt_terms = set(ground_truth.lower().split())\n",
    "    llm_terms = set(llm_output.lower().split())\n",
    "    return len(gt_terms.intersection(llm_terms)) > 1\n",
    "\n",
    "def get_critic_gpt_evaluation(ground_truth_row, llm_output_row):\n",
    "    \"\"\"Get RAGA evaluation from GPT for a single comparison\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Compare these statements:\n",
    "        Ground Truth: {ground_truth_row}\n",
    "        LLM Output: {llm_output_row}\n",
    "        \n",
    "        Based on the ground truth, evaluate the correctness of the LLM output in terms of:\n",
    "        1. Retrieval: Score from 0 to 1 for relevance to ground truth\n",
    "        2. Augmentation: Score from 0 to 1 for information synthesis\n",
    "        3. Generation: Score from 0 to 1 for fluency and grammar\n",
    "        4. Attribution: Score from 0 to 1 for factual attribution\n",
    "\n",
    "        Format your response exactly as:\n",
    "        Retrieval: X.XX\n",
    "        Augmentation: X.XX\n",
    "        Generation: X.XX\n",
    "        Attribution: X.XX\n",
    "        Brief explanation (one sentence).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=0.3,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_single_output(llm_row, ground_truth_df):\n",
    "    \"\"\"Evaluate a single LLM output against all ground truth entries\"\"\"\n",
    "    llm_text = f\"{llm_row['subj']} {llm_row['rel']} {llm_row['obj']}\"\n",
    "    best_scores = {\n",
    "        'retrieval': -1,\n",
    "        'augmentation': -1,\n",
    "        'generation': -1,\n",
    "        'attribution': -1\n",
    "    }\n",
    "    best_evaluation = None\n",
    "    best_gt_row = None\n",
    "    \n",
    "    for _, gt_row in ground_truth_df.iterrows():\n",
    "        gt_text = f\"{gt_row['subj']} {gt_row['rel']} {gt_row['obj']}\"\n",
    "        \n",
    "        if not is_relevant(gt_text, llm_text):\n",
    "            continue\n",
    "            \n",
    "        evaluation = get_critic_gpt_evaluation(gt_text, llm_text)\n",
    "        \n",
    "        if evaluation:\n",
    "            raga_scores = extract_raga_scores(evaluation)\n",
    "            if raga_scores and sum(raga_scores.values()) > sum(best_scores.values()):\n",
    "                best_scores = raga_scores\n",
    "                best_evaluation = evaluation\n",
    "                best_gt_row = gt_text\n",
    "    \n",
    "    return {\n",
    "        'llm_output': llm_text,\n",
    "        'ground_truth': best_gt_row,\n",
    "        'evaluation': best_evaluation,\n",
    "        **best_scores\n",
    "    } if best_gt_row else None\n",
    "\n",
    "def evaluate_all_outputs(llm_output_df, ground_truth_df, max_workers=10):\n",
    "    \"\"\"Process all LLM outputs in parallel with progress bar\"\"\"\n",
    "    all_evaluations = []\n",
    "    total = len(llm_output_df)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_row = {\n",
    "            executor.submit(evaluate_single_output, row, ground_truth_df): i \n",
    "            for i, (_, row) in enumerate(llm_output_df.iterrows())\n",
    "        }\n",
    "        \n",
    "        with tqdm(total=total, desc=\"Evaluating RAGA scores\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_row):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result:\n",
    "                        all_evaluations.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return all_evaluations\n",
    "\n",
    "def main():\n",
    "    llm_output = pd.read_csv(\"NewRels_Skip3_increments.csv\")\n",
    "    ground_truth = pd.read_csv(\"ground_truth.csv\")\n",
    "\n",
    "    print(f\"Processing {len(llm_output)} LLM outputs against {len(ground_truth)} ground truth statements...\")\n",
    "\n",
    "    # Run evaluation\n",
    "    evaluations = evaluate_all_outputs(\n",
    "        llm_output_df=llm_output,\n",
    "        ground_truth_df=ground_truth,\n",
    "        max_workers=10\n",
    "    )\n",
    "\n",
    "    # Calculate average scores\n",
    "    avg_scores = {\n",
    "        metric: np.mean([e[metric] for e in evaluations if e[metric] != -1])\n",
    "        for metric in ['retrieval', 'augmentation', 'generation', 'attribution']\n",
    "    }\n",
    "\n",
    "    print(\"\\nAverage RAGA Scores:\")\n",
    "    for metric, score in avg_scores.items():\n",
    "        print(f\"{metric.capitalize()}: {score:.2f}\")\n",
    "\n",
    "    # Save results to DataFrame\n",
    "    results_df = pd.DataFrame(evaluations)\n",
    "    results_df.to_csv('raga_evaluation_results.csv', index=False)\n",
    "    print(f\"\\nResults saved to Results/raga_evaluation_results.csv\")\n",
    "    print(f\"Processed {len(evaluations)} valid evaluations\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
