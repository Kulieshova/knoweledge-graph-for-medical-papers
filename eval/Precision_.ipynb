{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd4f32da-0be6-4691-94f4-1ec6dfdf75c9",
   "metadata": {},
   "source": [
    "Precision: Calculating how many of the extracted relationships are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b5ea6a-6ba9-436d-af14-a0344167b005",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deepeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8062da-d77a-4947-b40d-c052b4361ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize Python porter stemmer\n",
    "ps = PorterStemmer()\n",
    "def lemmatize(sent):\n",
    "    return [ps.stem(word) for word in sent.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df503792-98b1-4152-8e12-a4b4b73aae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz,os\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#reading the pdf and hand annotation files\n",
    "def read_pdf(pdf_file):\n",
    "    start=False\n",
    "    sentences=[]\n",
    "    start_idx=0\n",
    "    with fitz.open(pdf_file) as pdf_file:\n",
    "        for page_index, page in enumerate(pdf_file):\n",
    "            text = page.get_text(\"text\").lower()\n",
    "            text=text.split(\". \")\n",
    "            sentences.extend(text)\n",
    "                \n",
    "    return sentences\n",
    "def read_files(root_dir, hand):\n",
    "    \n",
    "    lines=[]\n",
    "    for files in os.listdir(root_dir):\n",
    "        if files[-4:] != '.pdf':\n",
    "            continue\n",
    "        sentences = read_pdf(f\"{root_dir}/{files}\")\n",
    "        lines.extend(sentences)\n",
    "\n",
    "    # read in hand annotations\n",
    "    for p in hand.iterrows():\n",
    "        rel = p[1]['rel']\n",
    "        subj = p[1]['subj']\n",
    "        obj = p[1]['obj']\n",
    "        out=f\"{subj} {rel} {obj}\" \n",
    "        lines.append(out)\n",
    "\n",
    "\n",
    "    return lines\n",
    "\n",
    "#computing cosine similarity\n",
    "def vec(sentences):\n",
    "    # Encode sentences\n",
    "    embeddings = model.encode([sentences[0], sentences[1]])\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = util.cos_sim(embeddings[0], embeddings[1])\n",
    "    return similarity.item() # Value close to 1 indicates high similarity\n",
    "    \n",
    "#finding if the target string (relation triplet) is in the src (pdf + hand annotation)\n",
    "def find(target, src):\n",
    "    found=False\n",
    "    matching_sentence=\"\"\n",
    " \n",
    "    for idx,sentence in enumerate(src):\n",
    "        pred=\" \".join(lemmatize(target))\n",
    "        test=\" \".join(lemmatize(sentence))\n",
    "        cos = vec([pred,test])\n",
    "        if pred in test or cos > 0.7:\n",
    "            if cos >0.65 and cos < 0.7:\n",
    "                print(f\"Got a match for {pred }: {sentence}\")\n",
    "            elif cos <=0.65:\n",
    "                print(f\"Closest match to {pred} was {test}\")\n",
    "            found=True\n",
    "            st_idx=idx\n",
    "            matching_sentence=sentence\n",
    "            return found, matching_sentence, \n",
    "            \n",
    "    return found, matching_sentence\n",
    "sentences = read_files(\"Docs\",hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "380547bb-663f-461e-9a85-b81771cacbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a sentence from the pdf: pharmacotherapy, especially selective \n",
      "serotonin reuptake inhibitors antidepressants, remains the most frequent option \n",
      "for treating depression during the acute phase, while other promising pharmaco-\n",
      "logical options are still competing for the attention of practitioners\n",
      "\n",
      "\n",
      "Example from the hand annot. Duloxetine High efficiency forÂ  Neuropathic pain, fibromyalgia, anxiety\n"
     ]
    }
   ],
   "source": [
    "print(f\"Example of a sentence from the pdf: {sentences[4]}\\n\\n\")\n",
    "print(f\"Example from the hand annot. {sentences[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a6ff708a-a300-4195-93e8-50f9819c81e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#find target in hand annotations\n",
    "def find_in_hand(hand,target):\n",
    "    max_=-1.0\n",
    "    for row in hand.iterrows():\n",
    "        hand_rel = f\" {row[1]['subj']} {row[1]['rel']} {row[1]['obj']}\"\n",
    "        sub_h = row[1]['subj']\n",
    "        cos=vec([hand_rel, target])\n",
    "        \n",
    "\n",
    "        if cos > max_:\n",
    "            max_ = cos\n",
    "            best=hand_rel\n",
    "        \n",
    "        cos=vec([best, hand_rel])\n",
    "        if sub_h.lower() not in best.lower() or cos < 0.7:\n",
    "            print(f\"Closest match for {hand_rel} is {best} with {cos}\")\n",
    "            continue\n",
    "        else:\n",
    "            return best\n",
    "            #print(f\"Closest match for {hand_rel} is {best} with {vec([best, hand_rel])}\")\n",
    "            #print(rouge_([hand_rel], [best]))\n",
    "            \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "666f4386-c9ef-4cc3-9bed-c991284eadef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find a match for   selective serotonin reuptake inhibitors (ssris) side effects sexual and digestive issues, irritability, anxiety, insomnia, and headache\n",
      "Couldn't find a match for   all antidepressants side effects nausea, vomiting, sexual dysfunction, sedation, priapism, cardiotoxicity\n",
      "Couldn't find a match for   cognitive-behavioral therapy (cbt) first-line treatment mild to moderate major depressive disorder (mdd)\n",
      "Couldn't find a match for   monoamine oxidase inhibitors (maois) less commonly used major depressive disorder (mdd)\n",
      "Couldn't find a match for   interpersonal therapy (ipt) first-line treatment mild to moderate major depressive disorder (mdd)\n",
      "Couldn't find a match for   cognitive behavioral therapy (cbt) first-line treatment depression\n",
      "Couldn't find a match for   selective serotonin reuptake inhibitors (ssris) side effects sexual and digestive (nausea and loss of appetite), as well as irritability, anxiety, insomnia, and headache\n",
      "Couldn't find a match for   selective serotonin reuptake inhibitors (ssris) side effects sexual and digestive problems\n",
      "Couldn't find a match for   cognitive-behavioral therapy (cbt) overview involves modifying dysfunctional behavior and cognitions\n",
      "Couldn't find a match for   cognitive-behavioral therapy (cbt) first-line treatment mild to moderate major depressive disorder\n",
      "Couldn't find a match for   mirtazapine more effective duloxetine, fluoxetine, fluvoxamine, paroxetine, reboxetine\n",
      "Couldn't find a match for   major depressive disorder associated with high rate of suicidal behavior and mortality\n",
      "Couldn't find a match for   selective serotonin reuptake inhibitors (ssris) side effects sexual and digestive issues, irritability, anxiety, insomnia, headache\n",
      "Couldn't find a match for   cognitive-behavioral therapy (cbt) first-line treatment mild to moderate major depressive disorder\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m obj \u001b[38;5;241m=\u001b[39m p[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobj\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 20\u001b[0m found, match \u001b[38;5;241m=\u001b[39m find(out, sentences)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m found:\n\u001b[1;32m     22\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[57], line 52\u001b[0m, in \u001b[0;36mfind\u001b[0;34m(target, src)\u001b[0m\n\u001b[1;32m     50\u001b[0m pred\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatize(target))\n\u001b[1;32m     51\u001b[0m test\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(lemmatize(sentence))\n\u001b[0;32m---> 52\u001b[0m cos \u001b[38;5;241m=\u001b[39m vec([pred,test])\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m test \u001b[38;5;129;01mor\u001b[39;00m cos \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cos \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.65\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cos \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.7\u001b[39m:\n",
      "Cell \u001b[0;32mIn[57], line 39\u001b[0m, in \u001b[0;36mvec\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvec\u001b[39m(sentences):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# Encode sentences\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode([sentences[\u001b[38;5;241m0\u001b[39m], sentences[\u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Compute cosine similarity\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mcos_sim(embeddings[\u001b[38;5;241m0\u001b[39m], embeddings[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:623\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    620\u001b[0m features\u001b[38;5;241m.\u001b[39mupdate(extra_features)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 623\u001b[0m     out_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    624\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhpu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    625\u001b[0m         out_features \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(out_features)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/SentenceTransformer.py:690\u001b[0m, in \u001b[0;36mSentenceTransformer.forward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m     module_kwarg_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule_kwargs\u001b[38;5;241m.\u001b[39mget(module_name, [])\n\u001b[1;32m    689\u001b[0m     module_kwargs \u001b[38;5;241m=\u001b[39m {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys}\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs)\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/models/Transformer.py:385\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m features:\n\u001b[1;32m    383\u001b[0m     trans_features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m features[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 385\u001b[0m output_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrans_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    386\u001b[0m output_tokens \u001b[38;5;241m=\u001b[39m output_states[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# If the AutoModel is wrapped with a PeftModelForFeatureExtraction, then it may have added virtual tokens\u001b[39;00m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;66;03m# We need to extend the attention mask to include these virtual tokens, or the pooling will fail\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1155\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1142\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1143\u001b[0m     embedding_output,\n\u001b[1;32m   1144\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1152\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1153\u001b[0m )\n\u001b[1;32m   1154\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1155\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[1;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (sequence_output, pooled_output) \u001b[38;5;241m+\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m:]\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:748\u001b[0m, in \u001b[0;36mBertPooler.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    746\u001b[0m first_token_tensor \u001b[38;5;241m=\u001b[39m hidden_states[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    747\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(first_token_tensor)\n\u001b[0;32m--> 748\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(pooled_output)\n\u001b[1;32m    749\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pooled_output\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/activation.py:366\u001b[0m, in \u001b[0;36mTanh.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtanh(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from deepeval.metrics import ContextualRecallMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "import pandas as pd\n",
    "hand = pd.read_csv(\"../Results/ground_truth.csv\")\n",
    "pred_files = [\"../Results/NewRels_Skip3.csv\", \"../Results/NewRels_Skip2.csv\"]\n",
    "i=0\n",
    "for pred_file in pred_files:\n",
    "    preds = pd.read_csv(pred_file) \n",
    "    \n",
    "    for p in preds.iterrows():\n",
    "        i+=1\n",
    "        if i%100==0:\n",
    "            print(i)\n",
    "        ref = p[1]['ref']\n",
    "        rel = p[1]['rel']\n",
    "        subj = p[1]['subj']\n",
    "        obj = p[1]['obj']\n",
    "        out=f\"{subj} {rel} {obj}\"\n",
    "        \n",
    "        found, match = find(out, sentences)\n",
    "        if found:\n",
    "            score +=1\n",
    "        else:\n",
    "            print(\"Couldn't find a match for  \", out)\n",
    "    print(f\"Precision for {pred_file} is {score/len(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d4b601b4-e384-4895-abf6-56c7b3273ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall score: 85.84905660377359%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Recall score: {100*score/len(preds)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f5b04-e33e-49ba-868f-6bc211825f06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
