{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a77e85e-04c8-473e-a333-a30b924764de",
   "metadata": {},
   "source": [
    "# BioBERT - Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7125995d-b6e4-4812-ac63-91c24bb0c296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 entries...\n",
      "Processed 10 entries...\n",
      "Processed 20 entries...\n",
      "Processed 30 entries...\n",
      "Processed 40 entries...\n",
      "\n",
      "Evaluation Results:\n",
      "Total Rows: 50\n",
      "Correct Matches (Score >= 0.7): 50\n",
      "Accuracy: 1.000\n",
      "\n",
      "Results saved to semantic_accuracy_evaluation_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import traceback\n",
    "from collections import namedtuple\n",
    "import pandas as pd\n",
    "import openai\n",
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Choose either BioBERT or SciBERT\n",
    "model_name = \"dmis-lab/biobert-base-cased-v1.1\"  # BioBERT\n",
    "# model_name = \"allenai/scibert_scivocab_uncased\"  # SciBERT\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def encode_texts(subj, rel, obj, tokenizer):\n",
    "    # Concatenate subject, relation, and object into one input\n",
    "    input_text = f\"{subj} [SEP] {rel} [SEP] {obj}\"\n",
    "    encoded = tokenizer(\n",
    "        input_text,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=176,  # Adjust max_length based on your task\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return encoded\n",
    "def get_embeddings(encoded_input, model):\n",
    "    with torch.no_grad():  # No gradient computation\n",
    "        outputs = model(**encoded_input)\n",
    "        # Use the [CLS] token's representation as the embedding\n",
    "        embeddings = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, hidden_size)\n",
    "    return embeddings\n",
    "\n",
    "from torch.nn.functional import cosine_similarity\n",
    "\n",
    "def compute_similarity(triple1, triple2, tokenizer, model):\n",
    "    # Encode triples\n",
    "    encoded1 = encode_texts(triple1['subj'], triple1['rel'], triple1['obj'], tokenizer)\n",
    "    encoded2 = encode_texts(triple2['subj'], triple2['rel'], triple2['obj'], tokenizer)\n",
    "\n",
    "    # Generate embeddings\n",
    "    emb1 = get_embeddings(encoded1, model)\n",
    "    emb2 = get_embeddings(encoded2, model)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(emb1, emb2).item()\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "def create_row(df, idx):\n",
    "    \"\"\"Create a formatted string representation of a row.\"\"\"\n",
    "    row = df.iloc[idx]\n",
    "    subj = str(row['subj']).strip().lower()\n",
    "    rel = str(row['rel']).strip().lower()\n",
    "    obj = str(row['obj']).strip().lower()\n",
    "    return {'subj': subj, 'rel': rel, 'obj': obj}\n",
    "\n",
    "def semantic_similarity_score_biobert(triple1, triple2, tokenizer, model):\n",
    "    \"\"\"Compute similarity using BioBERT/SciBERT embeddings.\"\"\"\n",
    "    similarity = compute_similarity(triple1, triple2, tokenizer, model)\n",
    "    reason = f\"Cosine similarity: {similarity:.2f}\"\n",
    "    return similarity, reason\n",
    "    # Calculate component scores with weights\n",
    "    subj_score = term_similarity(text1_dict['subj'], text2_dict['subj'], weight=0.4)\n",
    "    rel_score = term_similarity(text1_dict['rel'], text2_dict['rel'], weight=0.3)\n",
    "    obj_score = term_similarity(text1_dict['obj'], text2_dict['obj'], weight=0.3)\n",
    "    \n",
    "    total_score = subj_score + rel_score + obj_score\n",
    "    \n",
    "    # Generate detailed reason\n",
    "    reasons = []\n",
    "    if subj_score > 0:\n",
    "        reasons.append(f\"Subject match ({subj_score:.2f})\")\n",
    "    if rel_score > 0:\n",
    "        reasons.append(f\"Relation match ({rel_score:.2f})\")\n",
    "    if obj_score > 0:\n",
    "        reasons.append(f\"Object match ({obj_score:.2f})\")\n",
    "    \n",
    "    reason = \" + \".join(reasons) if reasons else \"No significant matches\"\n",
    "    \n",
    "    return total_score, reason\n",
    "\n",
    "def evaluate_matches_with_accuracy(ground_truth, llm_output, tokenizer, model, similarity_threshold=0.5):\n",
    "    \"\"\"Evaluate matches between ground truth and LLM output with accuracy calculation.\"\"\"\n",
    "    best_matches = []\n",
    "    correct_matches = 0  # Count of matches above the similarity threshold\n",
    "    \n",
    "    for i in range(len(ground_truth)):\n",
    "        gt_row = create_row(ground_truth, i)\n",
    "        best_evaluation = {'score': 0, 'reason': 'No matches found', 'match_idx': -1}\n",
    "        \n",
    "        for j in range(len(llm_output)):\n",
    "            llm_row = create_row(llm_output, j)\n",
    "            score, reason = semantic_similarity_score_biobert(gt_row, llm_row, tokenizer, model)\n",
    "            \n",
    "            if score > best_evaluation['score']:\n",
    "                best_evaluation = {'score': score, 'reason': reason}\n",
    "        \n",
    "        # Check if the best score exceeds the similarity threshold\n",
    "        if best_evaluation['score'] >= similarity_threshold:\n",
    "            correct_matches += 1\n",
    "        \n",
    "        # Format results\n",
    "        gt_string = f\"{gt_row['subj']} {gt_row['rel']} {gt_row['obj']}\"\n",
    "        llm_string = f\"{create_row(llm_output, j)['subj']} {create_row(llm_output, j)['rel']} {create_row(llm_output, j)['obj']}\" if best_evaluation['score'] >= similarity_threshold else \"No match found\"\n",
    "        \n",
    "        match_result = {\n",
    "            'ground_truth': gt_string,\n",
    "            'best_llm_output': llm_string,\n",
    "            'best_evaluation_score': best_evaluation['score'],\n",
    "            'reason': best_evaluation['reason']\n",
    "        }\n",
    "        best_matches.append(match_result)\n",
    "        \n",
    "        # Print progress\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Processed {i} entries...\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    total_rows = len(ground_truth)\n",
    "    accuracy = correct_matches / total_rows\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Total Rows: {total_rows}\")\n",
    "    print(f\"Correct Matches (Score >= {similarity_threshold}): {correct_matches}\")\n",
    "    print(f\"Accuracy: {accuracy:.3f}\")\n",
    "    \n",
    "    # Save results to a DataFrame\n",
    "    result_df = pd.DataFrame(best_matches)\n",
    "    return result_df, accuracy\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Load your data\n",
    "        ground_truth = pd.read_csv(\"my_annotations.csv\")\n",
    "        llm_output = pd.read_csv(\"my_temp.csv\")\n",
    "        \n",
    "        # Remove any unnamed columns\n",
    "        if 'Unnamed: 0' in llm_output.columns:\n",
    "            llm_output = llm_output.drop(columns=['Unnamed: 0'])\n",
    "            \n",
    "        # Run evaluation\n",
    "        similarity_threshold = 0.7  # Define your threshold\n",
    "        results_df, accuracy = evaluate_matches_with_accuracy(ground_truth, \n",
    "            llm_output, \n",
    "            tokenizer, \n",
    "            model, \n",
    "            similarity_threshold\n",
    "        )\n",
    "        # Save results\n",
    "        results_df.to_csv(\"semantic_accuracy_evaluation_results.csv\", index=False)\n",
    "        print(\"\\nResults saved to semantic_accuracy_evaluation_results.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        print(traceback.format_exc())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0813d15-d5de-4399-b49f-cff81bfd8cdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
