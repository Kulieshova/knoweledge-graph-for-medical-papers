
<div align="center">
  <img src="assets/header.svg" alt="Knowledge Graph For Mental Health" width="800"/>
</div>


<div align="center">
 <h3><strong>This repository contains tools and methodologies for evaluating Knowledge Graphs (KGs) generated by Large Language Models (LLMs) against manually annotated ground truth data. It also includes a user interface (UI) for managing KG data, as well as prompt engineering and evaluation techniques.</strong></h3>
</div>


## üë• Our Team ‚ù§Ô∏è

<div align="center">

| [![Rishika](https://github.com/rishikasrinivas.png?size=100)](https://github.com/rishikasrinivas) | [![Nataliia](https://github.com/Kulieshova.png?size=100)](https://github.com/Kulieshova) | [![Anushka](https://github.com/anushkalimaye.png?size=100)](https://github.com/anushkalimaye) | [![Kymari](https://github.com/Kymari28.png?size=100)](https://github.com/Kymari28) | [![Fernanda](https://github.com/Fernandadeltoro.png?size=100)](https://github.com/Fernandadeltoro) |
|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|:-------------------------:|
| **Rishika Srinivas** <br> @rishikasrinivas | **Nataliia Kulieshova** <br> @Kulieshova | **Anushka Limaye** <br> @anushkalimaye | **Kymari Bratton** <br> @Kymari28 | **Fernanda Del Toro** <br> @Fernandadeltoro |

</div>

## üõ†Ô∏è Technologies Used

### Core Technologies
- **Languages:** Python, HTML, CSS, JavaScript  
- **Database:** MongoDB  
- **AI Integration:** OpenAI API
- **User Interface:** React with Cytoscape JS for graph visualization


## üìù Ground Truth Annotation  

Hand annotations were meticulously developed by team members, who manually reviewed every sentence across three provided PDFs. These annotations formed triplets structured as:  
- `subj` (Subject)  
- `relationship` (Relationship)  
- `obj` (Object)  

<div align="center">
üí´ User Interface (UI)
Features
1. Upload Files üì§
<p align="center">
  <img src="assets/uploading.gif" alt="File Upload Demo" width="50%">
</p>
Easily import PDFs, which are converted into Knowledge Graphs (KGs) that extract clinical entities and relationships.
2. Fetch Previous Graphs üîÑ
<p align="center">
  <img src="assets/fetching.gif" alt="Fetch Previous Graphs Demo" width="50%">
</p>
Retrieve saved Knowledge Graphs for continued analysis or updates.
3. Search and Highlight üîç
<p align="center">
  <img src="assets/searching.gif" alt="Search Functionality Demo" width="50%">
</p>
A search-first design lets users quickly locate nodes or relationships.<br>
Results are highlighted in orange and zoomed in for clarity.
4. Dynamic Visualization üìä
<p align="center">
  <img src="assets/visualization.gif" alt="Visualization Demo" width="50%">
</p>

Nodes represent clinical entities, and edges use color coding and varying thickness<br>
to show relationship categories and strength.<br>
The Relationship Table offers a legend with clickable colored circles<br>
for more details on each relationship.<br>
A magnitude table shows the significance of relationships.

5. Custom Filtering üéØ
<p align="center">
  <img src="assets/filtering.gif" alt="Filtering Demo" width="50%">
</p>
Users can filter the graph to focus on specific relationship types<br>
(e.g., "Side Effects" or "Recommendations"), improving clarity without clutter.
6. Help Button ‚ÑπÔ∏è
<p align="center">
  <img src="assets/help.gif" alt="Search Functionality Demo" width="50%">
</p>
An intuitive Help button offers guidance on the graph's features,<br>
ensuring accessibility for new users and clinicians unfamiliar with knowledge graphs.
</div>

### Design Highlights ‚ú®

- **Brightside Health Branding**: The design aligns with **Brightside Health's brand** using calming blues, pastels, and creative accents like **color-coded edges** for a clean, engaging experience.
- **User-Centered Design**: Focuses on usability with:
  - **Interactive Relationship Table** for easy data interpretation
  - **Edge Thickness** to prioritize strong relationships for evidence-based decisions
  - **Search and Filtering** for focused, efficient navigation

## üìä Evaluation Methods

### Evaluation Metrics Table

| **Method**               | **Type**       | **Accuracy** | **Key Pitfalls**                   |
|---------------------------|----------------|--------------|-------------------------------------|
| Fuzzy Wuzzy              | Statistical    | 35.32%       | Low accuracy, only simple matches. |
| TF-IDF + Cosine Similarity | Statistical    | 36.28%       | Limited to vectorized text formats.|
| GPT Critic               | Model-Based    | 61.66%       | Requires large computational resources.|
| G-Eval                   | Model-Based    | TBD          | Requires large computational resources.    |
| PyTorch + bioBERT        | Model-Based    | TBD          | TBD    |

### Detailed Evaluation Methodologies

[Rest of your detailed evaluation methodologies content remains exactly the same...]

### Detailed Evaluation Methodologies  

#### 1. **Fuzzy Wuzzy**  
- **Evaluation Type:** Statistical  
- **Method:**  
   - Compare each row of the ground truth to each row of LLM output.  
   - Threshold for ‚Äúmatching‚Äù requires 70% or above similarity.  
- **Accuracy:** 35.32%  
- **Output:**  
   - Rows of LLM output that match the ground truth at or above 70% similarity.  
   - Only one triplet pair is found matching per threshold.  

---

#### 2. **TF-IDF Vector and Cosine Method**  
- **Evaluation Type:** Statistical, Feature-weighting  
- **Method:**  
   - Combine the triplet columns into a single string.  
   - Vectorize text using TF-IDF to convert it to numeric form.  
   - Compare each LLM row to each ground truth row using cosine similarity.  
- **Accuracy:** 36.28%  
- **Output:** Best matching ground truth row for each LLM output row.  

---

#### 3. **GPT Critic Parallel Batch Request**  
- **Evaluation Type:** Model-Based  
- **Method:**  
   - Uses 10 worker threads to enable parallel comparisons.  
   - Compares each LLM output row with ground truth rows using GPT-4 (RLHF).  
   - Finds the best similarity score for each LLM output.  
- **Accuracy:** 61.66%  
- **Output:** Best ground truth match for each LLM output row.  

---

#### 4. **G-Eval**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD  

---

#### 5. **PyTorch + bioBERT**  
- **Evaluation Type:** Model-Based  
- **Method:** TBD  
- **Threshold for Matching:** TBD  
- **Accuracy:** TBD  
- **Output:** TBD

#### 6. **Precision** 
- **Evaluation Type:** Statistical: word match and cosine similarity 
- **Method:** Checking if the extracted relationship is in the source text or in the ground truth annotations  
- **Threshold for Matching:** 0.7 
- **Precision Score:** 85.85

#### 7. **Hallucination** 
- **Evaluation Type:** Statistical: Factual Alignment and Consistency
- **Method:** DeepEval Hallutionation Metric
- **Threshold for Matching:** 0.5
- **Hallucination Score:** 84


---

## Contribution  
We welcome contributions to improve the evaluation methods, refine the UI, or expand the dataset. Please feel free to submit issues or pull requests.  

