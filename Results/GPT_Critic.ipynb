{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f94c98-cdbe-4b1a-8787-823dd648596c",
   "metadata": {},
   "source": [
    "# GPT Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8fc40a9-cd57-4d8c-a30f-394e1bc39e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 110 LLM outputs against 176 ground truth statements...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating relationships: 100%|█████████████████████████████████████████████████████████████████████████████████| 110/110 [03:23<00:00,  1.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Average Score: 0.69\n",
      "Processed 106 evaluations\n",
      "\n",
      "Summary DataFrame:\n",
      "                                          llm_output  \\\n",
      "0  venlafaxine comparable to tricyclic antidepres...   \n",
      "1  selective serotonin reuptake inhibitors (ssris...   \n",
      "2  mirtazapine properties increases serotonin or ...   \n",
      "3  selective serotonin reuptake inhibitors (ssris...   \n",
      "4  vortioxetine specific efficacy in treating dep...   \n",
      "\n",
      "                          best_matching_ground_truth  \\\n",
      "0  Monoamine oxidase inhibitors Same efficacy as ...   \n",
      "1  Norepinephrine reuptake inhibitors Eligible fo...   \n",
      "2  Mirtazapine Increases Availability of serotoni...   \n",
      "3  Pharmacotherapy Entails Norepinephrine reuptak...   \n",
      "4  Problem-solving therapy Use Preventing depress...   \n",
      "\n",
      "                                     best_evaluation  best_score  \n",
      "0  Score: 0.75\\nThe LLM output is accurate in ide...        0.75  \n",
      "1  Score: 0.50\\nThe LLM output is only partially ...        0.50  \n",
      "2  Score: 0.90\\nThe LLM output is almost identica...        0.90  \n",
      "3  Score: 0.50\\nThe LLM output is partially corre...        0.50  \n",
      "4  Score: 0.50\\nThe LLM output is somewhat relate...        0.50  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import re\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key='')\n",
    "\n",
    "def extract_score(evaluation):\n",
    "    \"\"\"Extract numerical score from GPT evaluation\"\"\"\n",
    "    match = re.search(r'(?:Overall correctness score|Score|I would score the LLM output a)[:\\s]*([0-1](?:\\.\\d+)?)', evaluation, re.IGNORECASE)\n",
    "    return float(match.group(1)) if match else -1\n",
    "\n",
    "def is_relevant(ground_truth, llm_output):\n",
    "    \"\"\"Check if LLM output is relevant to ground truth\"\"\"\n",
    "    gt_terms = set(ground_truth.lower().split())\n",
    "    llm_terms = set(llm_output.lower().split())\n",
    "    return len(gt_terms.intersection(llm_terms)) > 1\n",
    "\n",
    "def get_critic_gpt_evaluation(llm_text, gt_text):\n",
    "    \"\"\"Get evaluation from GPT for a single comparison\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Compare these statements:\n",
    "        Ground Truth: {gt_text}\n",
    "        LLM Output: {llm_text}\n",
    "        \n",
    "        Score from 0 to 1 for overall correctness (1 being highly correct). Format: 'Score: X.XX'\n",
    "        Brief explanation (one sentence).\n",
    "        \"\"\"\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=150\n",
    "        )\n",
    "        return response.choices[0].message.content.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error during evaluation: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_single_output(llm_row, ground_truth_df):\n",
    "    \"\"\"Evaluate a single LLM output against all ground truth entries\"\"\"\n",
    "    llm_text = f\"{llm_row['subj']} {llm_row['rel']} {llm_row['obj']}\"\n",
    "    best_score = -1\n",
    "    best_evaluation = None\n",
    "    best_gt_row = None\n",
    "    \n",
    "    for _, gt_row in ground_truth_df.iterrows():\n",
    "        gt_text = f\"{gt_row['subj']} {gt_row['rel']} {gt_row['obj']}\"\n",
    "        \n",
    "        if not is_relevant(gt_text, llm_text):\n",
    "            continue\n",
    "            \n",
    "        evaluation = get_critic_gpt_evaluation(llm_text, gt_text)\n",
    "        \n",
    "        if evaluation:\n",
    "            score = extract_score(evaluation)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_evaluation = evaluation\n",
    "                best_gt_row = gt_text\n",
    "    \n",
    "    return {\n",
    "        'llm_output': llm_text,\n",
    "        'best_matching_ground_truth': best_gt_row,\n",
    "        'best_evaluation': best_evaluation,\n",
    "        'best_score': best_score\n",
    "    }\n",
    "\n",
    "def evaluate_all_outputs(llm_output_df, ground_truth_df, max_workers=10):\n",
    "    \"\"\"Process all LLM outputs in parallel\"\"\"\n",
    "    all_evaluations = []\n",
    "    total = len(llm_output_df)\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Create future tasks\n",
    "        future_to_row = {\n",
    "            executor.submit(evaluate_single_output, row, ground_truth_df): i \n",
    "            for i, (_, row) in enumerate(llm_output_df.iterrows())\n",
    "        }\n",
    "        \n",
    "        # Process results with progress bar\n",
    "        with tqdm(total=total, desc=\"Evaluating relationships\") as pbar:\n",
    "            for future in concurrent.futures.as_completed(future_to_row):\n",
    "                try:\n",
    "                    result = future.result()\n",
    "                    if result['best_score'] != -1:\n",
    "                        all_evaluations.append(result)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing row: {e}\")\n",
    "                pbar.update(1)\n",
    "    \n",
    "    return all_evaluations\n",
    "\n",
    "# Load data\n",
    "llm_output = pd.read_csv(\"Results/NewRels_Skip3_increments.csv\")\n",
    "ground_truth = pd.read_csv(\"Results/ground_truth.csv\")\n",
    "\n",
    "print(f\"Processing {len(llm_output)} LLM outputs against {len(ground_truth)} ground truth statements...\")\n",
    "\n",
    "# Run evaluation\n",
    "evaluations = evaluate_all_outputs(\n",
    "    llm_output_df=llm_output,\n",
    "    ground_truth_df=ground_truth,\n",
    "    max_workers=10\n",
    ")\n",
    "\n",
    "# Calculate average score\n",
    "valid_scores = [eval['best_score'] for eval in evaluations if eval['best_score'] != -1]\n",
    "average_score = np.mean(valid_scores) if valid_scores else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nOverall Average Score: {average_score:.2f}\")\n",
    "print(f\"Processed {len(evaluations)} evaluations\")\n",
    "\n",
    "# Save results to DataFrame\n",
    "results_df = pd.DataFrame(evaluations)\n",
    "print(\"\\nSummary DataFrame:\")\n",
    "print(results_df.head())\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv('../Results/GPT_critic_llm_evaluation_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc37011-0d07-4438-bdd3-591457368e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
